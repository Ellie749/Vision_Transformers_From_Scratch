# Vision Transformers from Scratch

This repository contains a from-scratch implementation of the Vision Transformer (ViT) architecture using Tensorflow. It walks through the core building blocks such as positional encoding, patch embedding, multi-head attention, and classification head.

## ğŸ“ Project Structure
VISION_TRANSFORMERS_FROM_SCRATCH
| â”œâ”€â”€ examples
â”‚ â”œâ”€â”€ src 
â”‚   â”œâ”€â”€ data_pipeline 
â”‚   â”‚   â””â”€â”€ load_dataset.py # Functions to load and preprocess datasets 
â”‚   â”œâ”€â”€ model
â”‚   |    â””â”€â”€ train.py # Functions run experiments 
â”‚   â”œâ”€â”€ network
â”‚   â”‚   â”œâ”€â”€ architecture.py # Creating and assembling ViT network architecture
â”‚   â”‚   |   positional_encoding.py # Positional encoding for ViT 
â”‚   â”‚   â””â”€â”€ mlp.py
â”‚   â”œâ”€â”€  visualization
â”‚   â”‚   â””â”€â”€ utils.py # visualizing train metrics
â”‚   â”œâ”€â”€ weights/ # Directory to save trained model weights 
|   â”œâ”€â”€ experiments.ipynb # Notebook for quick experimentation
|   â”œâ”€â”€ project.p # main training code
| â”œâ”€â”€ application.py 
| â”œâ”€â”€ config.ini # Configuration file 
| â”œâ”€â”€ README.md # You're here!
| â””â”€â”€ requirement.txt



## ğŸš€ Features

- ğŸ”¢ Patch embedding and positional encoding
- ğŸ§  Transformer blocks (multi-head attention, MLP)
- ğŸ“¦ Data pipeline for loading and preprocessing image datasets
- ğŸ§ª Jupyter Notebook for testing components interactively
- ğŸ·ï¸ Modular codebase for easy extension

## ğŸ› ï¸ Requirements

Install dependencies via:

```bash
pip install -r requirements.txt


ğŸ“Š Dataset
Currently supports image classification datasets like CIFAR-10. You can modify load_dataset.py to plug in other datasets.

ğŸ§  TODO