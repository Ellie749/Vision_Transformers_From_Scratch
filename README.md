# Vision Transformers from Scratch

This repository contains a from-scratch implementation of the Vision Transformer (ViT) architecture using Tensorflow. It walks through the core building blocks such as positional encoding, patch embedding, multi-head attention, and classification head.

## ğŸ“ Project Structure
VISION_TRANSFORMERS_FROM_SCRATCH/ â”‚ â”œâ”€â”€ src/ â”‚ â”œâ”€â”€ data_pipeline/ â”‚ â”‚ â””â”€â”€ load_dataset.py # Functions to load and preprocess datasets â”‚ â”œâ”€â”€ model/ â”‚ â”‚ â””â”€â”€ network/ â”‚ â”‚ â””â”€â”€ positional_encoding.py # Positional encoding for ViT â”‚ â””â”€â”€ visualization/ # (Optional) Visual tools or helpers â”‚ â”œâ”€â”€ weights/ # Directory to save trained model weights â”œâ”€â”€ examples/ # Example training/inference scripts â”œâ”€â”€ application.py # Main entry point â”œâ”€â”€ project.py # Project-specific logic (e.g., training loop) â”œâ”€â”€ experiments.ipynb # Notebook for quick experimentation â”œâ”€â”€ config.ini # Configuration file â”œâ”€â”€ README.md # You're here!

